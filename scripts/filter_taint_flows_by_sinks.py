# -*- coding: utf-8 -*-

"""
	Copyright (C) 2022  Soheil Khodayari, CISPA
	This program is free software: you can redistribute it and/or modify
	it under the terms of the GNU Affero General Public License as published by
	the Free Software Foundation, either version 3 of the License, or
	(at your option) any later version.
	This program is distributed in the hope that it will be useful,
	but WITHOUT ANY WARRANTY; without even the implied warranty of
	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
	GNU Affero General Public License for more details.
	You should have received a copy of the GNU Affero General Public License
	along with this program.  If not, see <http://www.gnu.org/licenses/>.


	Description:
	------------
	Gets the relevant (i.e., request hijacking) flows from the set of dynamic taint flows generated by Foxhound
	Creates a JSON file containing the relevant flows in each webpage folder

	Running:
	------------
	$ python3 -m scripts.filter_taint_flows_by_sinks --input=/path/to/sitelist_crawled.csv --outputs=taintflows_relevant.json

"""

import os, sys
import json
import argparse
import pandas as pd

import utils.io as IOModule
import constants as constantsModule

from utils.logging import logger as LOGGER
import utils.utility as utilityModule



def get_relevant_taint_flows(unique_taintflows):
	
	target_sinks = [
		# web sockets
		"WebSocket",
		"WebSocket.send",
		# server side events
		"EventSource",
		# async requests (including push notification requests)
		"fetch.url",
		"fetch.body",
		"XMLHttpRequest.send",
		"XMLHttpRequest.open(url)",
		"XMLHttpRequest.setRequestHeader",
		# top level requests
		"Window.open",
		"location.href",
		"location.assign",
		"location.replace",
		# requests sent to fetch scripts
		"script.src"
	]

	target_sinks = [item.lower() for item in target_sinks]
	out_flows = []
	for taintflow in unique_taintflows:
		sink = taintflow["sink"].lower()
		for target_sink in target_sinks:
			if target_sink in sink:
				out_flows.append(taintflow)
				break

	return out_flows

def get_unique_objects(list_of_jsons):

	output = []
	tmp = []
	for item in list_of_jsons:
		hashed = str(item)
		if hashed not in tmp:
			tmp.append(hashed)
			output.append(item)

	return output

def get_unique_taint_flows(json_content):

	taintflows = get_unique_objects(json_content)

	unique_taintflows = []
	for taintflow_object in taintflows:
		taints = get_unique_objects(taintflow_object["taint"])	

		# merge multiple begin, end flows together
		merged_begin_end_taints = []
		tmp = []
		for t in taints:
			obj = {"flow": t["flow"]}
			hashed = str(obj)
			if hashed not in tmp:
				tmp.append(hashed)
				obj["begin"] = [t["begin"]]
				obj["end"] = [t["end"]]
				merged_begin_end_taints.append(obj)
			else:
				idx = tmp.index(hashed)
				obj = merged_begin_end_taints[idx]
				obj["begin"].append(t["begin"])
				obj["end"].append(t["end"])		
				merged_begin_end_taints[idx] = obj

		copy = taintflow_object
		copy["taint"] = merged_begin_end_taints
		unique_taintflows.append(copy)

	return unique_taintflows


def process_taint_flows(out_taintflows_path_name, json_content):
	
	unique_taintflows = get_unique_taint_flows(json_content)
	relevant_taintflows = get_relevant_taint_flows(unique_taintflows)

	try:
		with open(out_taintflows_path_name, 'w+') as fd:
			json.dump(relevant_taintflows, fd, ensure_ascii=False, indent=4)
	except Exception as e:
		LOGGER.warning(e) # UnicodeEncodeError

	t = len(json_content)
	m = len(unique_taintflows)
	n = len(relevant_taintflows)

	return [n, m, t]

def main():

	INPUT_FILE_NAME_DEFAULT = 'sitelist_crawled.csv'
	OUTPUTS_FILE_NAME_DEFAULT = 'taintflows_relevant.json'

	p = argparse.ArgumentParser(description='This script clusters webpages based on their similarly.')
	p.add_argument('--input', "-I",
		  metavar="FILE",
		  default=INPUT_FILE_NAME_DEFAULT,
		  help='list of sites (default: %(default)s)',
		  type=str)

	p.add_argument('--outputs', "-O",
		  metavar="FILE",
		  default=OUTPUTS_FILE_NAME_DEFAULT,
		  help='file name for the output (default: %(default)s)',
		  type=str)

	args= vars(p.parse_args())
	input_file_name = args["input"]
	outputs_file_name = args["outputs"]


	if input_file_name == INPUT_FILE_NAME_DEFAULT:
		input_file_name = os.path.join(os.path.join(constantsModule.BASE_DIR, "input"), INPUT_FILE_NAME_DEFAULT)


	LOGGER.info('started processing the taint flows.')
	
	data = {} # app folder name -> {page1: [n_relevant_flows, m_total_unique_flows, t_total_flows], page2: [...], ...}
	chunksize = 10**5
	for chunk_df in pd.read_csv(input_file_name, chunksize=chunksize, usecols=[0, 1], header=None, skip_blank_lines=True):
		for (index, row) in chunk_df.iterrows():
			etld_url = row[1]
			url = 'http://' + etld_url
			app_name = utilityModule.getDirectoryNameFromURL(url)
			app_path_name = os.path.join(constantsModule.DATA_DIR, app_name)
			if os.path.exists(app_path_name) and os.path.isdir(app_path_name):
				files = os.listdir(app_path_name)
				if len(files) > 1:
					data[app_name]= {}
					for webpage_name in files:
						webpage_path_name = os.path.join(app_path_name, webpage_name)
						if os.path.exists(webpage_path_name) and os.path.isdir(webpage_path_name):
							taintflows_json_file = os.path.join(webpage_path_name, "taintflows.json")
							if os.path.exists(taintflows_json_file):
								json_content = {}
								try:
									fd = open(taintflows_json_file, 'r')
									json_content = json.load(fd)
									fd.close()
								except:
									continue

								taintflows_output_path_name = os.path.join(webpage_path_name, OUTPUTS_FILE_NAME_DEFAULT)
								[n, m, t] = process_taint_flows(taintflows_output_path_name, json_content)
								data[app_name][webpage_name] = [n, m, t]
	
	with open(os.path.join(os.path.join(constantsModule.BASE_DIR, "input"), "taintflows_count.json"), 'w+') as fd:
		json.dump(data, fd, ensure_ascii=False, indent=4)

	LOGGER.info('finished.')


main()









